{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b130f2-c013-47d0-8626-083f0f64cb61",
   "metadata": {},
   "source": [
    "## Práctico 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ecef2a-e013-4d24-b967-acf1b4165d60",
   "metadata": {},
   "source": [
    "**\"Transform pattern\"** en el contexto de Machine Learning se refiere a la técnica de manipular y cambiar los datos de entrada antes de que sean utilizados por un modelo. \n",
    "\n",
    "Estos cambios pueden ayudar a mejorar el rendimiento del modelo y a hacer que el modelo sea más robusto ante variaciones en los datos de entrada.\n",
    "\n",
    "Las transformaciones se aplican típicamente durante la etapa de preprocesamiento de los datos y pueden implicar muchas técnicas diferentes, dependiendo del tipo de datos y del problema que se está tratando de resolver.\n",
    "\n",
    "En el caso de las imágenes, los patrones de transformación comunes incluyen la normalización (donde se ajusta la escala de los valores de los píxeles), la rotación, el cambio de tamaño, el recorte, la inversión horizontal/vertical (flip), el cambio de color, el zoom, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99af7356-b9cd-4173-9960-cbb284d4e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 98s 245ms/step - loss: 1.9489 - accuracy: 0.3383 - val_loss: 2.0219 - val_accuracy: 0.2769\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 93s 239ms/step - loss: 1.5622 - accuracy: 0.4387 - val_loss: 1.5027 - val_accuracy: 0.4560\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 94s 239ms/step - loss: 1.4226 - accuracy: 0.4898 - val_loss: 1.3785 - val_accuracy: 0.5162\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 94s 240ms/step - loss: 1.3465 - accuracy: 0.5185 - val_loss: 1.2867 - val_accuracy: 0.5350\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 95s 243ms/step - loss: 1.2914 - accuracy: 0.5438 - val_loss: 1.2129 - val_accuracy: 0.5573\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 94s 240ms/step - loss: 1.2478 - accuracy: 0.5589 - val_loss: 1.2467 - val_accuracy: 0.5568\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 95s 243ms/step - loss: 1.2134 - accuracy: 0.5719 - val_loss: 1.2516 - val_accuracy: 0.5722\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 92s 235ms/step - loss: 1.1796 - accuracy: 0.5846 - val_loss: 1.2481 - val_accuracy: 0.5729\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 89s 228ms/step - loss: 1.1570 - accuracy: 0.5946 - val_loss: 1.0362 - val_accuracy: 0.6337\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 90s 230ms/step - loss: 1.1414 - accuracy: 0.5997 - val_loss: 1.0232 - val_accuracy: 0.6394\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar y normalizar el conjunto de datos \n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convertir las etiquetas en one-hot\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define el preprocesamiento de la imagen\n",
    "data_augmentation = Sequential([\n",
    "    preprocessing.Rescaling(1./255),\n",
    "    preprocessing.RandomRotation(0.15),\n",
    "    preprocessing.RandomFlip(\"horizontal\"),\n",
    "    preprocessing.RandomZoom(0.2)\n",
    "])\n",
    "\n",
    "# Define tu modelo\n",
    "model = tf.keras.models.Sequential([\n",
    "    data_augmentation,  # Agrega las capas de preprocesamiento al inicio del modelo\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compila y entrena el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Guarda el modelo\n",
    "model.save('transform_pattern_conv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138333e-114a-47b4-928e-b24150df3100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0a6482f-172d-45d3-b7bf-00043ed6629a",
   "metadata": {},
   "source": [
    "En este caso, las transformaciones de datos **(rescaling, rotación aleatoria, ancho y altura aleatorios, flip horizontal, y zoom)** se definen como capas Keras y se agregan al inicio de tu modelo. \n",
    "\n",
    "Esto significa que estas transformaciones se aplicarán automáticamente a las imágenes a medida que pasen por el modelo, ya sea durante el entrenamiento o durante la inferencia. \n",
    "\n",
    "Además, como las capas de preprocesamiento son parte del modelo, se guardarán junto con el modelo cuando lo guardes con model.save(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d97697-1330-4217-8d4b-f3e1fef30dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
