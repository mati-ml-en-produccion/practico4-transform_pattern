{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b130f2-c013-47d0-8626-083f0f64cb61",
   "metadata": {},
   "source": [
    "## Práctico 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ecef2a-e013-4d24-b967-acf1b4165d60",
   "metadata": {},
   "source": [
    "**\"Transform pattern\"** en el contexto de Machine Learning se refiere a la técnica de manipular y cambiar los datos de entrada antes de que sean utilizados por un modelo. \n",
    "\n",
    "Estos cambios pueden ayudar a mejorar el rendimiento del modelo y a hacer que el modelo sea más robusto ante variaciones en los datos de entrada.\n",
    "\n",
    "Las transformaciones se aplican típicamente durante la etapa de preprocesamiento de los datos y pueden implicar muchas técnicas diferentes, dependiendo del tipo de datos y del problema que se está tratando de resolver.\n",
    "\n",
    "En el caso de las imágenes, los patrones de transformación comunes incluyen la normalización (donde se ajusta la escala de los valores de los píxeles), la rotación, el cambio de tamaño, el recorte, la inversión horizontal/vertical (flip), el cambio de color, el zoom, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af7356-b9cd-4173-9960-cbb284d4e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 03:08:11.922065: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-29 03:08:17.179036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-29 03:08:29.258298: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 89s 217ms/step - loss: 2.0257 - accuracy: 0.3031 - val_loss: 2.0731 - val_accuracy: 0.2683\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 85s 217ms/step - loss: 1.6595 - accuracy: 0.3993 - val_loss: 1.4568 - val_accuracy: 0.4662\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 1.5577 - accuracy: 0.4377 - val_loss: 1.4433 - val_accuracy: 0.4853\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 86s 220ms/step - loss: 1.4742 - accuracy: 0.4692 - val_loss: 1.3387 - val_accuracy: 0.5201\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 88s 226ms/step - loss: 1.4142 - accuracy: 0.4950 - val_loss: 1.4595 - val_accuracy: 0.4939\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 87s 221ms/step - loss: 1.3577 - accuracy: 0.5142 - val_loss: 1.3626 - val_accuracy: 0.5206\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 87s 222ms/step - loss: 1.3190 - accuracy: 0.5326 - val_loss: 1.4076 - val_accuracy: 0.5210\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 85s 218ms/step - loss: 1.2901 - accuracy: 0.5415 - val_loss: 1.1812 - val_accuracy: 0.5835\n",
      "Epoch 9/10\n",
      "295/391 [=====================>........] - ETA: 20s - loss: 1.2653 - accuracy: 0.5556"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar y normalizar el conjunto de datos \n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convertir las etiquetas en one-hot\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define el preprocesamiento de la imagen\n",
    "\n",
    "data_augmentation = Sequential([\n",
    "    preprocessing.Rescaling(1./255),\n",
    "    preprocessing.RandomRotation(0.15),\n",
    "    preprocessing.RandomFlip(\"horizontal\"),\n",
    "    preprocessing.RandomZoom(0.2),\n",
    "    preprocessing.RandomTranslation(0.1, 0.1),\n",
    "    preprocessing.RandomContrast(0.2),\n",
    "    preprocessing.RandomCrop(30, 30)\n",
    "])\n",
    "\n",
    "# Define tu modelo\n",
    "model = tf.keras.models.Sequential([\n",
    "    data_augmentation,  # Agrega las capas de preprocesamiento al inicio del modelo\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compila y entrena el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Guarda el modelo\n",
    "model.save('transform_pattern_conv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8138333e-114a-47b4-928e-b24150df3100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0a6482f-172d-45d3-b7bf-00043ed6629a",
   "metadata": {},
   "source": [
    "En este caso, las transformaciones de datos:\n",
    "- Rescaling(1./255),\n",
    "- RandomRotation(0.15),\n",
    "- RandomFlip(\"horizontal\"),\n",
    "- RandomZoom(0.2),\n",
    "- RandomTranslation(0.1, 0.1),\n",
    "- RandomContrast(0.2),\n",
    "- RandomCrop(30, 30))\n",
    "\n",
    "Se definen como capas Keras y se agregan al inicio del modelo. \n",
    "\n",
    "Esto significa que estas transformaciones se aplicarán automáticamente a las imágenes a medida que pasen por el modelo, ya sea durante el entrenamiento o durante la inferencia. \n",
    "\n",
    "Además, como las capas de preprocesamiento son parte del modelo, se guardarán junto con el modelo cuando lo guardes con model.save(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d97697-1330-4217-8d4b-f3e1fef30dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3caa3f7-81ce-4f4c-8662-7cfb3aa62bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
